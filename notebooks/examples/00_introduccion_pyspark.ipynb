{"cells":[{"cell_type":"markdown","id":"52834bcf","metadata":{"id":"52834bcf"},"source":["# Introducción a PySpark\n","\n","En este primer notebook utilizaremos Spark de forma local a través de PySpark. El objetivo es empezar a conocer la filosofía de su funcionamiento através de los comandos básicos.\n","\n","### Instalación\n","Tenemos varias opciones para tilizar Spark de forma local:\n","\n","1. En nuestra PC, crear un ambiente nuevo de python e instalar Spark, por ejemplo utilizando conda:\n","    * Crear el ambiente:\n","    __conda create -n env_pyspark__\n","    * Activar el ambiente:\n","    __conda activate env_pyspark__\n","    * Instalar pyspark:\n","    __conda install -c conda-forge pyspark ipykernel__\n","    * (opcional) Instalar librerias extra\n","\n","2. Utilizarlo en colab e instalarlo de la siguiente manera:\n","__!pip install pyspark__\n"]},{"cell_type":"code","execution_count":null,"id":"4e018418-a59f-472c-988e-5d275f2b9dd4","metadata":{"id":"4e018418-a59f-472c-988e-5d275f2b9dd4"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"markdown","id":"ae9fc605","metadata":{"id":"ae9fc605"},"source":["### Iniciar una sesión en Spark\n","\n","El primer paso al trabajar con Spark es iniciar una sesión. Lo podemos hacer de la siguiente forma:"]},{"cell_type":"code","execution_count":null,"id":"35b59058","metadata":{"id":"35b59058"},"outputs":[],"source":["#--importamos el objeto para cargar la sesion\n","from pyspark.sql import SparkSession\n","\n","#--inicializamos la sesion\n","spark = (SparkSession.builder\n","  #-especificamos que es local y que utilizamos todos los cores disponibles *\n","  .master('local[*]')\n","  #-podemos asignar un nombre a la sesión\n","  .appName('hello_world_spark')\n","  .getOrCreate())\n","\n","#--probamos que spark se haya inicializado\n","spark"]},{"cell_type":"markdown","id":"0f750d21","metadata":{"id":"0f750d21"},"source":["## Spark DataFrames\n","\n","PySpark dataframes son, a simple vista, similares a los de Pandas. Sin embargo a bajo nivel su implementación es totalmente diferente, su construcción esta basada en RDDs.\n","\n","Todas las transformaciones que se aplican a un dataframe son evaluadas de manera \"lazy\" hasta que alguna __acción__ se manda a llamar explícitamente. Esto permite que se concatenen varias transformaciones y se optimicen en conjunto, para llegar al resultado final.\n","\n","__Definicion__ dataframe en spark: tablas distribuidas en memoria cuya construcción contiene un esquema y nombres de columnas.\n","\n","Cada columna guarda un tipo especifico de dato, como: entero, character, array, etc.\n","\n","### Crear un data frame"]},{"cell_type":"code","execution_count":null,"id":"2132d7b4","metadata":{"id":"2132d7b4"},"outputs":[],"source":["#--importamos librerías de python para definir fechas\n","from datetime import datetime, date\n","#--objeto de pyspark para definir filas de un dataframe\n","from pyspark.sql import Row\n","\n","df = spark.createDataFrame([\n","    Row(a=1, b=2.0, c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n","    Row(a=2, b=3.0, c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n","    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n","])\n","\n","#--imprimimos\n","df"]},{"cell_type":"markdown","id":"448e5a4b","metadata":{"id":"448e5a4b"},"source":["El __schema__ es la parte del dataframe que define el nombre de las columnas y su tipo. Sino lo especificamos, spark automaticamente tratara de inferirlo.\n","\n","Es recomendable especificar el __schema__ siempre que sea posible porque:\n","\n","* Es computacionalmente costoso inferirlo en dataframes \"grandes\".\n","* Ayuda a detectar error: si un data es de un tipo diferente al esperado.\n","\n","Existen dos formas de definir un \"schema\", uno alineado a la formalidad de la programación y otro alienado más a la semantica humana. Un ejemplo de segundo seria:\n","\n","&nbsp;&nbsp;&nbsp; schema=\"name_col1 TIPO_DATO, name_col1 TIPO_DATO\"\n","\n","A continuación vemos la creación del mismo data frame con definición de \"schema\"."]},{"cell_type":"code","execution_count":null,"id":"b44711ed","metadata":{"id":"b44711ed"},"outputs":[],"source":["df = spark.createDataFrame([\n","    (1, 2.0, 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n","    (2, 3.0, 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n","    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n","],\n","    schema='a LONG, b DOUBLE, c STRING, d DATE, e TIMESTAMP')\n","df"]},{"cell_type":"markdown","id":"98079b19","metadata":{"id":"98079b19"},"source":["Otra forma de crear un data frame en spark es importarlo desde pandas o desde archivos RDD:\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__spark.createDataFrame(pandas_df)__\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__spark.createDataFrame(rdd_list_tuples)__\n","\n","y finalmente podemos guardar un __spark dataframe__ de regreso a pandas con:\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__df.toPandas()__"]},{"cell_type":"markdown","id":"86519b5f","metadata":{"id":"86519b5f"},"source":["### Visualización de un dataframe\n","\n","Para visualizar un dataframe y su esquema podemos usar:"]},{"cell_type":"code","execution_count":null,"id":"cd6ec27c","metadata":{"id":"cd6ec27c"},"outputs":[],"source":["df.show(2)"]},{"cell_type":"markdown","id":"ef05facd","metadata":{"id":"ef05facd"},"source":["Si queremos ver el dataframe de forma vertical:"]},{"cell_type":"code","execution_count":null,"id":"22c1217c","metadata":{"id":"22c1217c"},"outputs":[],"source":["#--si alguna columna tiene información mucha información, podemos imprimir de forma vertical\n","df.show(1, vertical=True)"]},{"cell_type":"code","execution_count":null,"id":"355fb449","metadata":{"id":"355fb449"},"outputs":[],"source":["#--para visualizar el esquema\n","df.printSchema()"]},{"cell_type":"markdown","id":"05028421","metadata":{"id":"05028421"},"source":["Al igual que en pandas, tambien existe en spark la funcion __describe()__ y __summary()__ la cual nos da un resumen de las estadísticas del dataset:"]},{"cell_type":"code","execution_count":null,"id":"16d3d666","metadata":{"id":"16d3d666"},"outputs":[],"source":["#--\"describe\" solo recibe como argumento las columnas a las cuales calcular\n","# la descripción\n","#df.describe(['a', 'b']).show()\n","\n","#--\"summary\" recibe como argumento las estadísticas a calcular\n","#df.summary(['mean', '99%', 'max']).show()\n","\n","#--podemos utilizar \"select\" para seleccionar las columnas antes del summary\n","df.select(['a','b']).summary(['mean', '95%', '99%', 'max']).show()"]},{"cell_type":"markdown","id":"755560ac","metadata":{"id":"755560ac"},"source":["\"show()\" es un método solamente para __imprimir__. Si deseamos hacer una rebanada (slice) de las primeras n filas o las últimas n filas podemos utilizar:\n","\n","* __take()__\n","* __tail()__\n","* __first()__\n","* __limit()__"]},{"cell_type":"code","execution_count":null,"id":"ad91ff88","metadata":{"id":"ad91ff88"},"outputs":[],"source":["#--para las primeras n filas\n","df.take(3)"]},{"cell_type":"code","execution_count":null,"id":"46603547","metadata":{"id":"46603547"},"outputs":[],"source":["#--para las últimas n filas\n","df.tail(2)"]},{"cell_type":"code","source":["#--también existe el método \"first\" para traer la primera fila que no es nula\n","df.first()"],"metadata":{"id":"X3NHEyMgU1_Z"},"id":"X3NHEyMgU1_Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--A diferencia de las demás, nos regresa un dataFrame\n","df.limit(4).show()"],"metadata":{"id":"H_L2Kz72VUSA"},"id":"H_L2Kz72VUSA","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ea3f8855","metadata":{"id":"ea3f8855"},"source":["Existe una acción más llamada __collect()__, la cual concentra todo los resultados de la transformación y las trae a la memoria local. Debe utilizarse con cuidado porque si el dataset es demasiado grande habrá un desbordamiento de memoria.\n","\n","Esta instrucción es útil cuando calculamos sumarizados de los datos y los deseamos traer a la memoria local para analizarlos o hacer gráficas."]},{"cell_type":"code","execution_count":null,"id":"374ebd25","metadata":{"id":"374ebd25"},"outputs":[],"source":["df.collect()"]},{"cell_type":"markdown","id":"b979be30","metadata":{"id":"b979be30"},"source":["### Selección de subsets en un dataframe\n","\n","En esta sección ejemplificamos como obtener un subset de columnas y/o filas del dataset.\n","\n","Estas transformaciones son evaluadas de forma \"lazy\" hasta que una acción, e.g. \"show\", es mandada a llamar"]},{"cell_type":"code","execution_count":null,"id":"f34d6b8d","metadata":{"id":"f34d6b8d"},"outputs":[],"source":["#--accesar al nombre de las columnas\n","df.columns"]},{"cell_type":"markdown","id":"3b46b270","metadata":{"id":"3b46b270"},"source":["En PySpark las columnas son objetos con métodos públicos. Se representan con el tipo: __Column__. Aunque cada columna es un objeto, no puede exister por si misma. Cada columna es parte de un fila, y todas las filas constituyen un dataframe."]},{"cell_type":"markdown","id":"a082a394","metadata":{"id":"a082a394"},"source":["La selección de columnas la podemos realizar:"]},{"cell_type":"code","execution_count":null,"id":"4a7676c1","metadata":{"id":"4a7676c1"},"outputs":[],"source":["#--acceder a una sola columna\n","df.a\n","\n","#--básicamente esta notación la utilizamos cuanda hacemos una\n","# selección anidada, por ejemplo para filtrar filas usando\n","# condicionales"]},{"cell_type":"markdown","id":"mAiFoYVCZpZ9","metadata":{"id":"mAiFoYVCZpZ9"},"source":["La otra forma es usando el método __select__:"]},{"cell_type":"code","execution_count":null,"id":"f56bd552","metadata":{"id":"f56bd552"},"outputs":[],"source":["#--la siguiente línea imprime el resultado debido a que tenemos activo\n","#     spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n","# Esta característica solo esta disponible en notebooks\n","#df.select('a')\n","\n","# Si quisieramos imprimir en otro ambiente que no sea notebooks, utilizariamos:\n","#df.select('a').show()\n","\n","# Si son varias columnas, utilizamos una lista\n","# df.select(['a','c'])\n","\n","# También, podemos utilizar \"my_dataFrame.nombre_columna\" dentro de \"select\":\n","df.select(df.a).show()"]},{"cell_type":"markdown","id":"478c728c","metadata":{"id":"478c728c"},"source":["Selección de un subset de filas se puede hacer utilizando __filter__ o __where__:"]},{"cell_type":"code","execution_count":null,"id":"97e8350f","metadata":{"id":"97e8350f"},"outputs":[],"source":["#--usando filter\n","#df.filter(df.a == 1)\n","df.filter(df.a == 1).show()"]},{"cell_type":"code","execution_count":null,"id":"303ba99c","metadata":{"id":"303ba99c"},"outputs":[],"source":["#--usando where\n","df.where(df.a == 1).show()"]},{"cell_type":"markdown","id":"58df598a","metadata":{"id":"58df598a"},"source":["### Agregar nuevas columnas al dataframe\n","\n","Para agregar columnas se utiliza la funcion __withColumn__, el primer argumento es el nombre de la nueva columna y el segundo, la columna a agregar.\n","\n","Si utilizamos el nombre de una columna existente, la columna se sustituye."]},{"cell_type":"code","execution_count":null,"id":"2bfb0fd3","metadata":{"id":"2bfb0fd3"},"outputs":[],"source":["from pyspark.sql.functions import upper\n","\n","#--agregamos una columna que contenga la misma informacion que \"c\" pero en\n","#-letras mayusculas\n","df.withColumn('upper_c', upper(df.c)).show()"]},{"cell_type":"markdown","source":["El modulo \"__pyspark.sql.functions__\" contiene muchas de las funciones que utilizamos para procesar los datos. Es una convención utilizar el siguiente import:"],"metadata":{"id":"l0NlwjZhXO2Y"},"id":"l0NlwjZhXO2Y"},{"cell_type":"code","source":["import pyspark.sql.functions as F"],"metadata":{"id":"E9PLKMIIXNgg"},"id":"E9PLKMIIXNgg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Otra forma de agregar columnas es con \"select\":"],"metadata":{"id":"3TvQAUACzc6d"},"id":"3TvQAUACzc6d"},{"cell_type":"code","source":["df.select('*', F.upper(df.c).alias('upper_c')).show()"],"metadata":{"id":"sacSNhRBzDw1"},"id":"sacSNhRBzDw1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¿Cuál es la diferencia entre las dos formas?\n","\n","Podemos ver las diferencias en el plan que Spark ejecutara utilizado el comando __explain()__:"],"metadata":{"id":"-o9VO70dz7BY"},"id":"-o9VO70dz7BY"},{"cell_type":"code","source":["df.select('*', upper(df.c).alias('upper_c')).explain()"],"metadata":{"id":"6xNU935ozkdq"},"id":"6xNU935ozkdq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.withColumn('upper_c', upper(df.c)).explain()"],"metadata":{"id":"AI6UM8BPztDK"},"id":"AI6UM8BPztDK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show()"],"metadata":{"id":"GqdYKmjpvyqy"},"id":"GqdYKmjpvyqy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cast double a int\n","double_cols = ['a', 'b', 'c']\n","my_df = df\n","for my_col in double_cols:\n","  my_df = (my_df\n","           .withColumn(my_col, F.upper(F.col(my_col))))\n","\n","my_df.explain()\n"],"metadata":{"id":"RabqY-AO87Wp"},"id":"RabqY-AO87Wp","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d61c2109","metadata":{"id":"d61c2109"},"source":["Es posible enlazar transformaciones y acciones. Por ejemplo, considere las siguientes acciones:\n","\n","* Agregar una columna nueva: concatenación de otras columnas\n","* Seleccionar la columna recién creada\n","* Mostrar las primeras dos filas"]},{"cell_type":"code","execution_count":null,"id":"286aaf26","metadata":{"id":"286aaf26"},"outputs":[],"source":["#-- expr: toma como argumento un string y lo convierte en nombre de columna\n","(df\n"," #-crear una nueva columna 'dca' que es la concatenacion de las columnas\n"," .withColumn('dca', (F.concat(F.expr('d'), df.c, F.expr('a *2'))))\\\n"," .select('dca')\n"," .show(2))\n"]},{"cell_type":"markdown","id":"1536472b","metadata":{"id":"1536472b"},"source":["### Leer y escribir archivos\n","\n","Spark ofrece varias opciones para leer archivos desde diferentes fuentes y formatos.\n","\n","Los archivos CSV se puede guardar y leer de la siguiente forma:"]},{"cell_type":"code","execution_count":null,"id":"b789a3d4","metadata":{"id":"b789a3d4"},"outputs":[],"source":["#--guardar un archivo a csv\n","df.write.csv('foo.csv', header=True, mode=\"overwrite\")"]},{"cell_type":"markdown","id":"62b204ba","metadata":{"id":"62b204ba"},"source":["La celda anterior guarda el archivo en un CSV que es distribuido. Si deseamos obtener un csv tradicional, tenemos que convertirlo a pandas dataframe:"]},{"cell_type":"code","execution_count":null,"id":"64b7f001","metadata":{"id":"64b7f001"},"outputs":[],"source":["import pandas as pd\n","\n","df.toPandas().to_csv('foo_pandas.csv', index=False)"]},{"cell_type":"markdown","id":"249b01ba","metadata":{"id":"249b01ba"},"source":["Para leer de CSV:"]},{"cell_type":"code","execution_count":null,"id":"fb6dcac5","metadata":{"id":"fb6dcac5"},"outputs":[],"source":["#--importante definir el \"schema\"\n","df = spark.read.csv('foo.csv', header =True,\n","                    schema='a LONG, b DOUBLE, c STRING, d DATE, e TIMESTAMP')\n","\n","df.show()"]},{"cell_type":"markdown","id":"6db25b51","metadata":{"id":"6db25b51"},"source":["Las primeras etapas de un proyecto de ciencia de datos consisten en explorar, limpiar y transformar los datos. Es recomendable, especialmente en grandes cantidades de datos, guardar una copia del dataset transformado para mantener los cambios y no calcularlos cada vez que iteramos.\n","\n","Aunque los CSV es un formato común __no es eficiente para leer y esribir en disco__. Por tanto, se recomienda utilizar formatos optimizados para estos fines como lo es el formato \"parquet\".\n","\n","__Notas__:\n","* Parquet no admite nombre de columnas con espacios.\n","* Cuando salvamos a parquet, se guarda el \"schema\". No es necesario volver a definirlo al leer de parquet."]},{"cell_type":"code","execution_count":null,"id":"a57e609b","metadata":{"id":"a57e609b"},"outputs":[],"source":["#--escribir nuestro dataframe a un archivo parquet\n","df.write.parquet('bar.parquet', mode=\"overwrite\")\n","\n","#--volver a leerlo\n","spark.read.parquet('bar.parquet').show()"]},{"cell_type":"markdown","id":"e41e02ec","metadata":{"id":"e41e02ec"},"source":["### SQL\n","\n","Spark SQL y dataframe funcionana sobre la misma infraestructura por lo que pueden utilizarse de forma intercambiable.\n","\n","Para utilizar SQL-queries creamos una \"vista temporal\" de nuestros datos y después podemos hacer \"queries\" como si fuera SQL."]},{"cell_type":"code","execution_count":null,"id":"914f6c40","metadata":{"id":"914f6c40"},"outputs":[],"source":["#--vista temporal\n","df.createOrReplaceTempView(\"table_A\")\n","\n","#--query\n","spark.sql(\"SELECT count(*) from table_A\").show()"]},{"cell_type":"markdown","id":"af389dd3","metadata":{"id":"af389dd3"},"source":["Otra forma es utilizar \"expr\" y el query que deseamos hacer:"]},{"cell_type":"code","execution_count":null,"id":"81c67e6a","metadata":{"id":"81c67e6a"},"outputs":[],"source":["df.select(F.expr('count(*)')).show()\n","\n","#--debido a que \"select\" y \"expr\" son muy utilizadas, existe un forma más corta:\n","df.selectExpr('count(*)').show()"]},{"cell_type":"markdown","id":"2571773e","metadata":{"id":"2571773e"},"source":["## Ejemplo 1: contar chocolates m&m\n","\n","Vamos a leer un archivo CSV que contiene la cuenta de chocolates m&m por color y por estado."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"J0ZZ-Ww1QzJv"},"id":"J0ZZ-Ww1QzJv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6e035465","metadata":{"id":"6e035465"},"outputs":[],"source":["\n","mnm_file = \"/content/drive/MyDrive/data_sets/mnm_dataset.csv\"\n","mnm_schema = 'State string, Color string, Count int'"]},{"cell_type":"markdown","id":"d8ed69d0","metadata":{"id":"d8ed69d0"},"source":["Otra forma distinta de leer CSV."]},{"cell_type":"code","execution_count":null,"id":"87e018d0","metadata":{"id":"87e018d0"},"outputs":[],"source":["#--leer el archivo csv\n","#-especificamos que es \"csv\", le decimos que tiene nombre de columnas\n","mnm_df = (spark.read.format(\"csv\")\n","    .option(\"header\", \"true\")\n","    # .option(\"inferSchema\", \"true\")\n","    .option('schema', mnm_schema)\n","    .load(mnm_file))"]},{"cell_type":"code","source":["mnm_df.show(5)"],"metadata":{"id":"g7LhREL-7xFU"},"id":"g7LhREL-7xFU","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"478b1a49","metadata":{"id":"478b1a49"},"source":["Supongamos que deseamos contar los m&m por estado y por color:"]},{"cell_type":"code","execution_count":null,"id":"4d707922","metadata":{"id":"4d707922"},"outputs":[],"source":["# 1. Seleccionamos las columnas de interes\n","# 2. agrupamos por estado y color\n","# 3. agregamos la cuenta\n","# 4. ordenamos de forma descendete\n","\n","#--en este ejemplo, utilizo () para agrupar todas las acciones\n","count_mnm_df = (mnm_df\n","    .select(\"State\", \"Color\", \"Count\")\n","    .groupBy(\"State\", \"Color\")\n","    .agg(F.sum(\"Count\").alias(\"Total\"))\n","    .orderBy(\"Total\", ascending=False))\n","\n","#--imprimimos los resultados\n","count_mnm_df.show(n=10, truncate=False)\n","print(f\"Total Rows = {count_mnm_df.count()}\")"]},{"cell_type":"markdown","source":["Todo procesamiento se puede hacer también utilizando sintaxis de SQL. Se debe crear una \"vista\" de la tabla y ya después se puede operar sobre esa vista."],"metadata":{"id":"5jF1d_Sld7dP"},"id":"5jF1d_Sld7dP"},{"cell_type":"code","source":["#--repetir el ejercicio anterior utilizando sintaxis SQL\n","mnm_df.createOrReplaceTempView(\"table_mnm\")\n","spark.sql(\"SELECT State, Color, count(*) as COUNT from table_mnm group by State, Color order by COUNT DESC\").show()"],"metadata":{"id":"M5WoJQ1QR3q-"},"id":"M5WoJQ1QR3q-","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b5aa542b","metadata":{"id":"b5aa542b"},"source":["Si solo deseamos saber el resultado del estado de California:"]},{"cell_type":"code","execution_count":null,"id":"eece62aa","metadata":{"id":"eece62aa"},"outputs":[],"source":["#--asumamos que deseamos ver solo datos de un solo estado\n","ca_count_mnm_df = mnm_df\\\n","    .select(\"State\", \"Color\", \"Count\")\\\n","    .where(mnm_df.State == \"CA\")\\\n","    .groupBy(\"State\", \"Color\")\\\n","    .agg(F.count(\"Count\").alias(\"Total\"))\\\n","    .orderBy(\"Total\", ascending=False)\n","\n","#--imprimimos\n","ca_count_mnm_df.show(n=5, truncate=False)"]},{"cell_type":"markdown","id":"e8ac11b9","metadata":{"id":"e8ac11b9"},"source":["## Ejemplo 2: San Francisco fire calls"]},{"cell_type":"markdown","id":"ee68facb","metadata":{"id":"ee68facb"},"source":["De nuevo vamos a leer un CSV, sin embargo esta vez vamos a definir el \"schema\" siguiendo una forma más estricta:"]},{"cell_type":"code","execution_count":null,"id":"5c66cc97","metadata":{"id":"5c66cc97"},"outputs":[],"source":["from pyspark.sql.types import *\n","\n","#--otra forma de definir un scheme\n","fire_schema = StructType([StructField(name='CallNumber', dataType=IntegerType(), nullable=True),\n","    StructField('UnitID', StringType(), True),\n","    StructField('IncidentNumber', IntegerType(), True),\n","    StructField('CallType', StringType(), True),\n","    StructField('CallDate', StringType(), True),\n","    StructField('WatchDate', StringType(), True),\n","    StructField('CallFinalDisposition', StringType(), True),\n","    StructField('AvailableDtTm', StringType(), True),\n","    StructField('Address', StringType(), True),\n","    StructField('City', StringType(), True),\n","    StructField('Zipcode', IntegerType(), True),\n","    StructField('Battalion', StringType(), True),\n","    StructField('StationArea', StringType(), True),\n","    StructField('Box', StringType(), True),\n","    StructField('OriginalPriority', StringType(), True),\n","    StructField('Priority', StringType(), True),\n","    StructField('FinalPriority', IntegerType(), True),\n","    StructField('ALSUnit', BooleanType(), True),\n","    StructField('CallTypeGroup', StringType(), True),\n","    StructField('NumAlarms', IntegerType(), True),\n","    StructField('UnitType', StringType(), True),\n","    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n","    StructField('FirePreventionDistrict', StringType(), True),\n","    StructField('SupervisorDistrict', StringType(), True),\n","    StructField('Neighborhood', StringType(), True),\n","    StructField('Location', StringType(), True),\n","    StructField('RowID', StringType(), True),\n","    StructField('Delay', FloatType(), True)])"]},{"cell_type":"markdown","id":"89adf896","metadata":{"id":"89adf896"},"source":["Leemos los datos"]},{"cell_type":"code","execution_count":null,"id":"9cf7834c","metadata":{"id":"9cf7834c"},"outputs":[],"source":["sf_fire_file = \"/content/drive/MyDrive/data_sets/sf-fire-calls.csv\"\n","fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n","fire_df.show(5, truncate=False)"]},{"cell_type":"markdown","id":"fe40dfc1","metadata":{"id":"fe40dfc1"},"source":["Asumimos que hicimos algún preprocesamiento a los datos, y vamos a proceser a guardar los cambios. Finalmente, leemos de nuevo el archivo."]},{"cell_type":"code","execution_count":null,"id":"fe45b997","metadata":{"id":"fe45b997"},"outputs":[],"source":["#--definimos donde queremos guarar el archivo\n","parquet_path = \"/content/drive/MyDrive/data_sets/sf_fire_calls\"\n","#--lo guardamos\n","fire_df.write.save(parquet_path, format=\"parquet\", mode=\"overwrite\")"]},{"cell_type":"code","execution_count":null,"id":"9918f692","metadata":{"id":"9918f692"},"outputs":[],"source":["#--volvemos a leer los datos, ahora desde el formato parquet\n","#--No necesitamos especificar el schema debido a que el formato parquet lo guarda\n","fire_df = spark.read.parquet(parquet_path)"]},{"cell_type":"markdown","id":"2644c995","metadata":{"id":"2644c995"},"source":["Selección de columnas y filas especificas la podemos hacer da l siguiente manera:"]},{"cell_type":"code","execution_count":null,"id":"9d77ef3a","metadata":{"id":"9d77ef3a"},"outputs":[],"source":["few_fire_df = (fire_df\n","    .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n","    .where(F.col(\"CallType\") != \"Medical Incident\"))\n","\n","few_fire_df.show(5)"]},{"cell_type":"markdown","id":"2081177d","metadata":{"id":"2081177d"},"source":["¿Cómo podríamos saber cuantos tipos de llamadas (CallTypes) fueron hechas?"]},{"cell_type":"code","execution_count":null,"id":"50e75030","metadata":{"id":"50e75030"},"outputs":[],"source":["(fire_df\n","    .select(\"CallType\") #-seleccionamos la columna 'CallType'\n","    .where(F.col(\"CallType\").isNotNull()) #-obtenemos solo las filas que no son nulas\n","    .agg(F.countDistinct(\"CallType\").alias(\"TiposDellamadas\")) #-contamos cuantos registros hay\n","    .show())"]},{"cell_type":"markdown","id":"5b1e86da","metadata":{"id":"5b1e86da"},"source":["Podemos listar los tipos de llama usando la función \"dropDuplicates\". A continuación solo mostramos las primeras 10:"]},{"cell_type":"code","execution_count":null,"id":"9529d520","metadata":{"id":"9529d520"},"outputs":[],"source":["(fire_df\n","    .select(\"CallType\")\n","    .where(F.col(\"CallType\").isNotNull())\n","    .dropDuplicates()\n","    .show(10, truncate=False))"]},{"cell_type":"markdown","id":"2856c141","metadata":{"id":"2856c141"},"source":["El nombre de las columnas se decidió al definir el \"schema\". Si deseamos cambiar un nombre de columna lo podemos lograr con:"]},{"cell_type":"code","execution_count":null,"id":"a0d8287e","metadata":{"id":"a0d8287e"},"outputs":[],"source":["#--cambiamos el nombre de la columna \"Delay\" por el de \"ResponseDelayedinMins\"\n","new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n","\n","#--seleccionamos las filas que tengan un retraso mayor a 5 mins e imprimimos las primeras 5 con\n","#-mayor retraso.\n","(new_fire_df\n","    .select(\"ResponseDelayedinMins\")\n","    .where(F.col(\"ResponseDelayedinMins\") > 5)\n","    .orderBy(F.col(\"ResponseDelayedinMins\"), ascending=False)\n","    .show(5, False))"]},{"cell_type":"markdown","id":"dcddfdfb","metadata":{"id":"dcddfdfb"},"source":["Algunas veces es necesario cambiar el tipo de dato de alguna columna. Como por ejemplo, las columnas \"CallDate\", \"WatchDate\" y \"AvailableDtTm\" son de tipo string pero contienen fechas."]},{"cell_type":"code","execution_count":null,"id":"e2e2f6d8","metadata":{"id":"e2e2f6d8"},"outputs":[],"source":["#--imprimimos que tipo de datos son las columnas\n","(new_fire_df\n"," .select('CallDate', 'WatchDate', 'AvailableDtTm')\n"," .printSchema())\n","#--imprimimos las primeras 5 filas\n","(new_fire_df\n"," .select('CallDate', 'WatchDate', 'AvailableDtTm')\n"," .show(5,False))\n"]},{"cell_type":"code","execution_count":null,"id":"cc619b0b","metadata":{"id":"cc619b0b"},"outputs":[],"source":["fire_ts_df = (new_fire_df\n","    .withColumn(colName=\"IncidentDate\", col=F.to_timestamp(F.col(\"CallDate\"), \"MM/dd/yyyy\"))\n","    .drop(\"CallDate\")\n","    .withColumn(\"OnWatchDate\", F.to_timestamp(F.col(\"WatchDate\"), \"MM/dd/yyyy\"))\n","    .drop(\"WatchDate\")\n","    .withColumn(\"AvailableDtTS\", F.to_timestamp(F.col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\"))\n","    .drop(\"AvailableDtTm\"))\n","\n","#--mostramos las columnas que cambiamos\n","(fire_ts_df\n",".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",".show(5, False))\n","\n","(fire_ts_df\n",".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",".printSchema())"]},{"cell_type":"markdown","id":"286800ef","metadata":{"id":"286800ef"},"source":["Con las columnas de tipo fecha podemos utilizar funciones como __month()__, __year()__, y __day()__. Por ejemplo, podemos ver todos los años donde se ha reportado algún incidente:"]},{"cell_type":"code","execution_count":null,"id":"b209c80d","metadata":{"id":"b209c80d"},"outputs":[],"source":["(fire_ts_df\n","    .select(F.year('IncidentDate'))\n","    .distinct()\n","    #.orderBy(F.year('IncidentDate').desc())\n","    .orderBy(F.year('IncidentDate').asc())\n","    .show())"]},{"cell_type":"markdown","id":"a37cd403","metadata":{"id":"a37cd403"},"source":["¿Cuáles es el tipo de incidente más común?"]},{"cell_type":"code","execution_count":null,"id":"9cb4edb6","metadata":{"id":"9cb4edb6"},"outputs":[],"source":["(fire_ts_df\n","    .select(\"CallType\") #-seleccionamos solo la columna de interes\n","    .where(F.col(\"CallType\").isNotNull()) #-seleccionamos las filas no nulas\n","    .groupBy(\"CallType\") #-las agrupamos por su tipo\n","    .count() #-contamos cuantas hay de cada tipo\n","    .orderBy(\"count\", ascending=False) #-ordenamos\n","    .show(n=10, truncate=False)) #-imprimimos"]},{"cell_type":"markdown","id":"8c5ef448","metadata":{"id":"8c5ef448"},"source":["Otras funciones comunes son __min()__, __max()__, __sum()__ y __avg()__. Para un mayor detalle ver la API \"pyspark.sql.functions\". Un ejemplo:"]},{"cell_type":"code","execution_count":null,"id":"337b0087","metadata":{"id":"337b0087"},"outputs":[],"source":["(fire_ts_df\n","    .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n","            F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n","    .show())"]},{"cell_type":"markdown","id":"4cd164f8","metadata":{"id":"4cd164f8"},"source":["Algunas otras funciones más especializadas:\n","\n","* abs()\n","* stat()\n","* describe()\n","* correlation()\n","* covariance()\n","* sampleBy()\n","* approxQuantile()\n","* frequentItems()\n","\n","Para mas información:\n","\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"]},{"cell_type":"markdown","id":"c6b5c81c","metadata":{"id":"c6b5c81c"},"source":["Una vez que terminamos de utilizar Spark es importante cerrar la sesion para liberar los recursos que se estan ocupando."]},{"cell_type":"code","execution_count":null,"id":"eb1b37ed","metadata":{"id":"eb1b37ed"},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"8e025801","metadata":{"id":"8e025801"},"outputs":[],"source":["spark"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"4589e6741dc5b8c26716a12c550dfae48c2d7c4e68c9bad9cf0c511312d00c2a"}}},"nbformat":4,"nbformat_minor":5}