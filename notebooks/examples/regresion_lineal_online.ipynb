{"cells":[{"cell_type":"markdown","metadata":{"id":"CbqVgcA51Nm4"},"source":["# Regresión lineal: online"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgs_3GbT1Nm5"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","#--para comparar nuestros resultados con la solución exacta\n","from sklearn.linear_model import LinearRegression\n","\n","#--para escalar los datos\n","from sklearn.preprocessing import StandardScaler\n","\n","#--para utilizar SGD de sklearn\n","from sklearn.linear_model import SGDRegressor"]},{"cell_type":"markdown","metadata":{"id":"1sSxiz-F1Nm6"},"source":["El objetivo es realizar una regresión lineal para explicar los niveles de ozono dada la temperatura utilizando el dataset \"airquality.csv\".\n","\n","Compararemos el resultado de la regresión lineal en línea con la solución \"exacta\" de scikit-learn. También exploraremos la función de regresión lineal basada en gradiente estocástico de scikit-learn."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"THj3nK136iyN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcR8J8yO1Nm6"},"outputs":[],"source":["#--cargamos todo el data set en memoria\n","database_file = '/content/drive/MyDrive/data_sets/airquality.csv'\n","dat = pd.read_csv(database_file)\n","dat.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmkVYWLB1Nm7"},"outputs":[],"source":["#--para utilizar gradient descent es necesario que los atributos tengan\n","#-las mismas escalas. Aunque en este ejemplo solo estamos utilizando\n","#-una variable por lo que la normalización no toma relevancia.\n","\n","#--definimos una instancia para normalizar los datos\n","escalar = StandardScaler()\n","\n","#--normalizamos los datos\n","dat_scaled = escalar.fit_transform(dat[['Temp']])\n","\n","print(dat_scaled[:5])"]},{"cell_type":"markdown","metadata":{"id":"0VKizVF91Nm8"},"source":["La variable respuesta no necesita ser normalizada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apFWiNV51Nm8"},"outputs":[],"source":["#--utilizando el dataset completo en memoria, calculamos la solución exacta\n","#-para después compararla con los métodos en línea.\n","\n","#--definimos una instancia de regresión lineal\n","modelo_exacto = LinearRegression()\n","#--entrenamos el modelo\n","modelo_exacto.fit(X = dat_scaled, y = dat['Ozone'])\n","#--observamos sus parámetros o coeficientes\n","print(f'theta_0: {np.round(modelo_exacto.intercept_,2)}')\n","print(f'theta_1: {np.round(modelo_exacto.coef_[0])}')\n","\n","#--graficamos el resultado\n","#-no dibujo todos los puntos, solo me interesa saber el inicio y el final\n","#-para unirlos por una línea\n","minimo = dat_scaled.min()\n","maximo = dat_scaled.max()\n","new_sample = np.array([[minimo], [maximo]])\n","y_hat_exacto = modelo_exacto.predict(new_sample)\n","\n","plt.plot(dat_scaled, dat['Ozone'], 'ko')\n","plt.plot(new_sample, y_hat_exacto, 'b-')\n","plt.title('Modelo exacto')\n","\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"Ki275Fhx1Nm8"},"source":["Solución usando batch gradient descent (se usa todo el data set para calcular una actualización a los parámetros).\n","\n","Asumimos que tenemos acceso a todo el dataset en memoria."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBWOewQm1Nm8"},"outputs":[],"source":["alpha = .0001    #--learning rate\n","epochs = 300    #--cuantas veces iteramos sobre TODO el dataset\n","\n","#--variable respuesta\n","y = dat[['Ozone']]\n","#--numero de muestras\n","m = y.shape[0]\n","#--agregamos una columna de unos a los atributos\n","X = np.ones((m, 2))\n","X[:,1:] = dat_scaled\n","#--inicializamos las thetas a cero\n","thetas = np.zeros((2,1))\n","#--iteramos el número de epocas definido previamente\n","for i in range(epochs):\n","    #--calculamos la predicción con las thetas actuales\n","    y_hat = np.dot(X, thetas)\n","    #--obtenemos el error\n","    error = y_hat - y\n","    #--actualizamos las thetas\n","    thetas = thetas - alpha * np.dot(X.T,error)\n","\n","#--creamos una muestra nueva con una columna de unos\n","new_sample_ones = np.ones((2,2))\n","new_sample_ones[:,1:] = new_sample\n","#--hacemos la predicción\n","y_hat = np.dot(new_sample_ones, thetas)\n","#--ploteamos los resultados\n","plt.plot(dat_scaled, dat['Ozone'], 'ko')\n","plt.plot(new_sample, y_hat_exacto, 'b-', label='exacto')\n","plt.plot(new_sample, y_hat, 'r--', label='gradient descent')\n","plt.legend()\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"oL6tPwpg1Nm9"},"source":["Utilizaremos __stochastic gradient descent__ (actualizamos los parámetros con cada muestra). Aún consideramos que tenemos el dataset completo en memoria."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVF91IGi1Nm9"},"outputs":[],"source":["alpha = .001    #--learning rate\n","epochs = 1000     #--cuantas veces iteramos sobre TODO el dataset\n","\n","#--variable respuesta\n","y = dat['Ozone']\n","#--número de muestras\n","m = y.shape[0]\n","#--agregamos una columna de unos al training set\n","X = np.ones((m, 2))\n","X[:,1:] = dat_scaled\n","#--definos las thetas como un vector de ceros\n","thetas = np.zeros((2,1))\n","#--iteramos sobre el número de epocas\n","for i in range(epochs):\n","    #--iteramos sobre cada muestra del training set\n","    for j in range(m):\n","        #--calculamos la predicción con las thetas actuales\n","        y_hat = np.dot(X[None,j,:], thetas)\n","        #--observamos el error\n","        error = y_hat - y[j]\n","        #--actualizamos las thetas. Observen que empezamos a\n","        #-actuilzar las thetas con la primera muesta que vemos,\n","        #-a diferencia de batch gradient descent que realiza\n","        #-su primera actualización, una vez que vio todo el dataset\n","        thetas = thetas - alpha * np.dot(X[None,j,:].T, error)\n","\n","\n","#--hacemos la predicción\n","y_hat = np.dot(new_sample_ones, thetas)\n","#--ploteamos los resultados\n","plt.plot(dat_scaled, dat['Ozone'], 'bo')\n","plt.plot(new_sample, y_hat_exacto, 'k-', label='exacto')\n","plt.plot(new_sample, y_hat, 'r--', label='stochastic gradient descent')\n","plt.legend()\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"7DqZe9L41Nm9"},"source":["Scikit-learn tiene una función que realiza la regresión lineal usando sotchastic gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfQ-76pR1Nm9"},"outputs":[],"source":["alpha = .01   #--learning rate\n","epochs = 30    #--cuantas veces iteramos sobre TODO el dataset\n","\n","m = dat_scaled.shape[0]\n","modelo_sklearn = SGDRegressor(eta0=alpha)\n","for i in range(epochs):\n","    for j in range(m):\n","        modelo_sklearn.partial_fit(X = dat_scaled[None, j, :],\n","                                   y = dat.loc[j,['Ozone']])\n","\n","new_sample = np.array([[minimo], [maximo]])\n","y_hat_sklearn = modelo_sklearn.predict(new_sample)\n","\n","plt.plot(dat_scaled, dat['Ozone'], 'bo')\n","plt.plot(new_sample, y_hat_exacto, 'k-')\n","plt.plot(new_sample, y_hat_sklearn, 'r--');"]},{"cell_type":"markdown","metadata":{"id":"4YW-VCdP1Nm-"},"source":["Finalmente, consideremos el caso donde no cargamos todo el dataset en memoria y utilizamos \"stochastic gradient descent\".\n","\n","Debemos conocer la media y la varianza de cada atributo para normalizar los datos.\n","* Podemos leer una vez el data set y calcular los parametros necesarios.\n","* Si previamente obtuvimos una muestra, la podemos utilizar para obtener los parámetros que necesitamos.\n","\n","En este ejemplo, vamos a leer el data set para calcular el promedio y la desviación estandar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOxbMNpl1Nm-"},"outputs":[],"source":["#--calculo del promedio y desviación estandar\n","\n","n_samples = 0\n","sum_x = 0\n","sum_x2 = 0\n","\n","reader = pd.read_csv(database_file, iterator = True)\n","try:\n","    #--iteramos hasta el final\n","    while reader:\n","        row = reader.get_chunk(1)\n","        sum_x += row.values[0,3] #--la columna 3 es Temp\n","        sum_x2 += row.values[0,3]**2\n","        n_samples += 1\n","except StopIteration:\n","    pass\n","finally:\n","    del reader\n","\n","promedio = sum_x / n_samples\n","var = sum_x2/n_samples - promedio**2\n","std = var**(1/2)\n","print(f'promedio: {np.round(promedio,2)}')\n","print(f'desviación estandard: {np.round(std,2)}')"]},{"cell_type":"markdown","source":["Finalmente calculamos la regresión lineal con los parámetros anteriores y leyendo el archivo poco a poco."],"metadata":{"id":"PJe-ZAuML1jl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Jrp19pU1Nm-"},"outputs":[],"source":["alpha = .001    #--learning rate\n","epochs = 30     #--cuantas veces iteramos sobre TODO el dataset\n","\n","\n","#--definos las thetas como un vector de ceros\n","thetas = np.zeros((2,1))\n","#--iteramos sobre el número de epocas\n","for i in range(epochs):\n","    #--iteramos sobre cada muestra del training set\n","    reader = pd.read_csv(database_file, iterator = True)\n","    try:\n","        #--iteramos hasta el final\n","        while reader:\n","            row = reader.get_chunk(1)\n","            temp = (row.values[0,3] - promedio) / std\n","            X = np.array([[1,temp]])\n","            y_hat = np.dot(X, thetas)\n","            #--observamos el error\n","            error = y_hat - row.values[0,0] #--la columna 0 es Ozone\n","            thetas = thetas - alpha * np.dot(X.T, error)\n","    except StopIteration:\n","        pass\n","    finally:\n","        del reader\n","\n","\n","#--hacemos la predicción\n","y_hat = np.dot(new_sample_ones, thetas)\n","#--ploteamos los resultados\n","plt.plot(dat_scaled, dat['Ozone'], 'bo')\n","plt.plot(new_sample, y_hat_exacto, 'k-', label='exacto')\n","plt.plot(new_sample, y_hat, 'r--', label='stochastic gradient descent')\n","plt.legend()\n","plt.show();"]},{"cell_type":"code","source":[],"metadata":{"id":"Re9XTPI0O35S"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}